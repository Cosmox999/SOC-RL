{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM1wWo+KtkzpSwhSZiXRdt1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cosmox999/SOC-RL/blob/main/sac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy==1.23.5 gym==0.25.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "sS_rNW8_E-sR",
        "outputId": "5b9ce85a-227f-41a5-ec63-0786d918c3df"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: gym==0.25.2 in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.25.2) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.25.2) (0.0.8)\n",
            "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.20.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "langchain 0.3.19 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.38.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.2.0 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "2a93fd4498404dbf9261a402db8eca3e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQq503YFEg5A",
        "outputId": "e9124b14-ccee-4de9-affe-7fcc3a0c2f4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, Reward: -1315.5934731809123\n",
            "Episode: 10, Reward: -1178.662697110516\n",
            "Episode: 20, Reward: -868.9250862488867\n",
            "Episode: 30, Reward: -1040.6500903699769\n",
            "Episode: 40, Reward: -647.0165651645594\n",
            "Episode: 50, Reward: -648.9576955947495\n",
            "Episode: 60, Reward: -1023.5574159953212\n",
            "Episode: 70, Reward: -735.725062390191\n",
            "Episode: 80, Reward: -792.0655109856966\n",
            "Episode: 90, Reward: -126.63417408262525\n",
            "Episode: 100, Reward: -123.71334244519775\n",
            "Episode: 110, Reward: -371.95395358083374\n",
            "Episode: 120, Reward: -247.07317151519567\n",
            "Episode: 130, Reward: -4.8028083386638185\n",
            "Episode: 140, Reward: -358.61009266040793\n",
            "Episode: 150, Reward: -123.4839473142723\n",
            "Episode: 160, Reward: -360.05168393455335\n",
            "Episode: 170, Reward: -123.53525797267784\n",
            "Episode: 180, Reward: -119.41148606879798\n",
            "Episode: 190, Reward: -240.90747643913244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/record_video.py:78: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment Pendulum-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "<ipython-input-3-b282235d19b1>:203: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  policy_net.load_state_dict(torch.load(\"sac_policy.pth\"))\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from gym.wrappers import RecordVideo\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(None)\n",
        "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = np.random.choice(len(self.buffer), batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*[self.buffer[idx] for idx in batch])\n",
        "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], dim=1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "\n",
        "        self.mean = nn.Linear(256, action_dim)\n",
        "        self.log_std = nn.Linear(256, action_dim)\n",
        "\n",
        "        self.action_scale = torch.tensor(2.0)\n",
        "        self.action_bias = torch.tensor(0.0)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "\n",
        "        mean = self.mean(x)\n",
        "        log_std = self.log_std(x)\n",
        "        log_std = torch.clamp(log_std, min=-20, max=2)\n",
        "        std = torch.exp(log_std)\n",
        "\n",
        "        dist = Normal(mean, std)\n",
        "        x_t = dist.rsample()\n",
        "        y_t = torch.tanh(x_t)\n",
        "        action = y_t * self.action_scale + self.action_bias\n",
        "        log_prob = dist.log_prob(x_t) - torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
        "        log_prob = log_prob.sum(-1, keepdim=True)\n",
        "\n",
        "        return action, log_prob\n",
        "\n",
        "def sac_train(env, episodes=150, batch_size=256, gamma=0.99, tau=0.005, alpha=0.2, lr=3e-4):\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "\n",
        "    # Networks\n",
        "    q_net1 = QNetwork(state_dim, action_dim)\n",
        "    q_net2 = QNetwork(state_dim, action_dim)\n",
        "    target_q_net1 = QNetwork(state_dim, action_dim)\n",
        "    target_q_net2 = QNetwork(state_dim, action_dim)\n",
        "    policy_net = PolicyNetwork(state_dim, action_dim)\n",
        "\n",
        "    # Copy parameters to target networks\n",
        "    target_q_net1.load_state_dict(q_net1.state_dict())\n",
        "    target_q_net2.load_state_dict(q_net2.state_dict())\n",
        "\n",
        "    # Optimizers\n",
        "    q_optimizer1 = optim.Adam(q_net1.parameters(), lr=lr)\n",
        "    q_optimizer2 = optim.Adam(q_net2.parameters(), lr=lr)\n",
        "    policy_optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
        "\n",
        "    # Replay buffer\n",
        "    replay_buffer = ReplayBuffer(100000)\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            action, _ = policy_net(state_tensor)\n",
        "            action = action.detach().numpy()[0]\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            replay_buffer.push(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if len(replay_buffer) > batch_size:\n",
        "                # Sample batch\n",
        "                states, actions, rewards_batch, next_states, dones = replay_buffer.sample(batch_size)\n",
        "\n",
        "                states = torch.FloatTensor(states)\n",
        "                actions = torch.FloatTensor(actions)\n",
        "                rewards_batch = torch.FloatTensor(rewards_batch).unsqueeze(1)\n",
        "                next_states = torch.FloatTensor(next_states)\n",
        "                dones = torch.FloatTensor(1 - dones).unsqueeze(1)\n",
        "\n",
        "                # Q targets\n",
        "                with torch.no_grad():\n",
        "                    next_actions, next_log_probs = policy_net(next_states)\n",
        "                    q1_target = target_q_net1(next_states, next_actions)\n",
        "                    q2_target = target_q_net2(next_states, next_actions)\n",
        "                    q_target = torch.min(q1_target, q2_target) - alpha * next_log_probs\n",
        "                    q_target = rewards_batch + gamma * dones * q_target\n",
        "\n",
        "                # Update Q networks\n",
        "                q1 = q_net1(states, actions)\n",
        "                q2 = q_net2(states, actions)\n",
        "\n",
        "                q1_loss = (q1 - q_target).pow(2).mean()\n",
        "                q2_loss = (q2 - q_target).pow(2).mean()\n",
        "\n",
        "                q_optimizer1.zero_grad()\n",
        "                q1_loss.backward()\n",
        "                q_optimizer1.step()\n",
        "\n",
        "                q_optimizer2.zero_grad()\n",
        "                q2_loss.backward()\n",
        "                q_optimizer2.step()\n",
        "\n",
        "                # Update policy network\n",
        "                actions_pi, log_probs = policy_net(states)\n",
        "                q1_pi = q_net1(states, actions_pi)\n",
        "                q2_pi = q_net2(states, actions_pi)\n",
        "                q_pi = torch.min(q1_pi, q2_pi)\n",
        "\n",
        "                policy_loss = (alpha * log_probs - q_pi).mean()\n",
        "\n",
        "                policy_optimizer.zero_grad()\n",
        "                policy_loss.backward()\n",
        "                policy_optimizer.step()\n",
        "\n",
        "                # Update target networks\n",
        "                for target_param, param in zip(target_q_net1.parameters(), q_net1.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "                for target_param, param in zip(target_q_net2.parameters(), q_net2.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            print(f\"Episode: {episode}, Reward: {total_reward}\")\n",
        "\n",
        "    # Save the policy network\n",
        "    torch.save(policy_net.state_dict(), \"sac_policy.pth\")\n",
        "\n",
        "    return rewards\n",
        "\n",
        "def plot_rewards(rewards, title=\"SAC Rewards\"):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(rewards)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Total Reward\")\n",
        "    plt.savefig(\"sac_rewards.png\")\n",
        "    plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a directory for videos\n",
        "    if not os.path.exists(\"videos\"):\n",
        "        os.makedirs(\"videos\")\n",
        "\n",
        "    # First train the agent\n",
        "    env = gym.make('Pendulum-v1')\n",
        "    rewards = sac_train(env, episodes=200)  # Reduced episodes for Colab demo\n",
        "    plot_rewards(rewards)\n",
        "\n",
        "    # Then record a video of the trained agent\n",
        "    env = RecordVideo(gym.make('Pendulum-v1'), \"videos\", name_prefix=\"sac\")\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    policy_net = PolicyNetwork(env.observation_space.shape[0], env.action_space.shape[0])\n",
        "    policy_net.load_state_dict(torch.load(\"sac_policy.pth\"))\n",
        "\n",
        "    while not done:\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        action, _ = policy_net(state_tensor)\n",
        "        state, _, done, _ = env.step(action.detach().numpy()[0])\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # In Colab, you can view the video with:\n",
        "    # from IPython.display import HTML\n",
        "    # import base64\n",
        "    #\n",
        "    # def show_video(video_path):\n",
        "    #     video_file = open(video_path, \"r+b\").read()\n",
        "    #     video_url = f\"data:video/mp4;base64,{base64.b64encode(video_file).decode()}\"\n",
        "    #     return HTML(f\"\"\"<video width=\"400\" controls><source src=\"{video_url}\"></video>\"\"\")\n",
        "    #\n",
        "    # video_path = \"videos/sac-episode-0.mp4\"\n",
        "    # show_video(video_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IIyhtrSsFIfu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}